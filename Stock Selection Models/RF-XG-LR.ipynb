{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as py\n",
    "import pandas as pd\n",
    "import matplotlib as pl\n",
    "import yfinance as yf\n",
    "from pandas_datareader import data as pdr\n",
    "from pandas_datareader import famafrench\n",
    "import datetime as dt\n",
    "#import ticker list file here\n",
    "namelist=pd.read_csv(\"Thesis/StockSelection/FINALSTOCK.csv\")\n",
    "namelist=namelist.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start=dt.datetime(2006,1,1)\n",
    "end=dt.datetime(2021,8,31)\n",
    "ticker=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load dataset initial \n",
    "for i in range(len(namelist.Ticker)):\n",
    "    data = pdr.get_data_yahoo(str(namelist.iloc[i,0]), start=start, end=end)#change date to 22 days before for vol\n",
    "    data=pd.DataFrame(data).drop('Volume',axis=1)\n",
    "    ticker.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas_datareader import data\n",
    "z=data.get_quote_yahoo(list(namelist['Ticker']))[['longName','region','marketCap']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.to_csv('stock_list.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#match date values \n",
    "for i in range(len(ticker)):\n",
    "    ticker[i]=ticker[i].reset_index()\n",
    "    ticker[i]=ticker[i].dropna()\n",
    "sets=list(set(ticker[i]['Date']) for i in range(len(ticker)))\n",
    "missin=pd.to_datetime(list(set.union(*sets)-set.intersection(*sets)),\n",
    "                         format=\"%Y%m%d\").sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add new row, sort,fillna(improve when have time)\n",
    "for i in range(len(ticker)):\n",
    "    for j in missin:\n",
    "        if j in list(ticker[i]['Date']):\n",
    "            pass\n",
    "        else:\n",
    "            ticker[i].loc[ticker[i].shape[0]]=[j,None,None,None,None,None]\n",
    "            ticker[i]=ticker[i].sort_values(by=['Date'],ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fillna:\n",
    "for i in range(len(ticker)):\n",
    "    ticker[i]=ticker[i].interpolate()\n",
    "    print(ticker[i].isna().sum())\n",
    "    print(ticker[i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickersave=ticker.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker=tickersave.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def risk_adjusted_return: fama french 5 factor\n",
    "fama=pd.read_csv('Thesis/LongShortPerformance/Fama.csv')\n",
    "fama['Date']=pd.to_datetime(fama['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tri=[]\n",
    "for i in range(len(ticker)):\n",
    "    ticker[i]['RF']=ticker[i].Date.map(fama.set_index('Date')['RF']).fillna(0)\n",
    "    ticker[i]=ticker[i].set_index('Date')\n",
    "    tri.append(py.log(ticker[i].loc[:, 'Open'] / ticker[i].loc[:, 'Close'].shift(1)) ** 2 +\n",
    "                                       0.5 *py.log(ticker[i].loc[:, 'High'] / ticker[i].loc[:, 'Low']) ** 2 -\n",
    "                                       (2 * py.log(2) - 1) *\n",
    "                                       py.log(ticker[i].loc[:, 'Close'] / ticker[i].loc[:, 'Open']) ** 2)\n",
    "    ticker[i]['Return']= ticker[i]['Adj Close'].pct_change(1)\n",
    "    ticker[i]['Excess Return']=ticker[i]['Return']-ticker[i]['RF']*0.01\n",
    "    ticker[i]['Volatility'] = py.sqrt(252 / 22 *\n",
    "                  pd.DataFrame.rolling(py.log(ticker[i].loc[:, 'Open'] / ticker[i].loc[:, 'Close'].shift(1)) ** 2 +\n",
    "                                       0.5 *py.log(ticker[i].loc[:, 'High'] / ticker[i].loc[:, 'Low']) ** 2 -\n",
    "                                       (2 * py.log(2)   - 1) *\n",
    "                                       py.log(ticker[i].loc[:, 'Close'] / ticker[i].loc[:, 'Open']) ** 2,\n",
    "                                       window=22).sum())\n",
    "    ticker[i]['Risk Adj Return']=ticker[i]['Excess Return']/ticker[i]['Volatility'].shift(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tri=pd.DataFrame(data=tri)\n",
    "tri=tri.T\n",
    "tri=tri.transform(lambda x:x**0.5)\n",
    "tri.columns=namelist['Ticker']\n",
    "tri.to_csv('Thesis/S336951_VolatilityConnectedness.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(ticker)):\n",
    "    ticker[i]=ticker[i][24:]#Volatility NaN and Risk Adj Returns Nan\n",
    "    ticker[i]=ticker[i].reset_index() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date=ticker[0].Date\n",
    "r=[]\n",
    "for i in range(93):\n",
    "    r.append(ticker[i]['Risk Adj Return'])\n",
    "r=pd.DataFrame(r).T\n",
    "r['Date']=date\n",
    "r.to_csv('Risk Adj Return.csv')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def avr_benchmark:FOR EACH DAYY!\n",
    "bm=[]\n",
    "for i in range(len(ticker[0]['Return'])):\n",
    "    dayavr=[]\n",
    "    for j in range(len(ticker)): \n",
    "        dayavr.append(ticker[j].iloc[i,10])\n",
    "    med_val=py.median(dayavr)\n",
    "    bm.append(med_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def performance_rating(ticker):\n",
    "for i in range(len(ticker)): \n",
    "    day_perf=[]\n",
    "    for j in range(len(ticker[0])):\n",
    "        if ticker[i].iloc[j,10] >= bm[j]:\n",
    "            day_perf.append(1)\n",
    "        else:    \n",
    "            day_perf.append(0)\n",
    "    ticker[i]['Performance']=day_perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check distribution and Set date index\n",
    "a=0\n",
    "for i in range(len(ticker)):\n",
    "    a=a+1\n",
    "    print(a)\n",
    "    #ticker[i]=ticker[i].set_index('Date')\n",
    "    print(ticker[i]['Performance'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "II. ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recent_data=ticker.copy()\n",
    "#fr=pd.read_csv('from.csv')\n",
    "#to=pd.read_csv('to.csv')\n",
    "net=pd.read_csv('net.csv')\n",
    "for i in range(len(recent_data)):\n",
    "    recent_data[i]=recent_data[i].iloc[196:-195,-7:]\n",
    "    #recent_data[i]=recent_data[i].reset_index()\n",
    "    #recent_data[i]=recent_data[i].drop('index',axis=1)\n",
    "    recent_data[i]=recent_data[i].drop('Return',axis=1)\n",
    "    recent_data[i]=recent_data[i].drop('Excess Return',axis=1)\n",
    "    recent_data[i]=recent_data[i].drop('Volatility',axis=1)\n",
    "    recent_data[i]=recent_data[i].drop('RF',axis=1)\n",
    "    recent_data[i]=recent_data[i].drop('Adj Close',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(recent_data)):\n",
    "    recent_data[i].Performance=recent_data[i].Performance.shift(-5)\n",
    "    recent_data[i]=recent_data[i].iloc[:-7,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "recent_data[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p=[]\n",
    "for i in range(len(recent_data)):\n",
    "    p.append(recent_data[i].Performance)\n",
    "pd.DataFrame(p).T.to_csv('Thesis/Performance1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Recent Training- WALK FORWARD\n",
    "# Might need to slice data for the slack- update later\n",
    "def split_X_y_ticker(j):\n",
    "    for i in range(90):\n",
    "        #Split train,test\n",
    "        split=recent_data[j].iloc[42*i:252+42*(i+1),:]\n",
    "        y=py.array(split.Performance)\n",
    "        X=py.array(split.drop('Performance',axis=1))\n",
    "        scale=StandardScaler()\n",
    "        X=scale.fit_transform(X)\n",
    "        X_tr=X[:-42]\n",
    "        y_tr=y[:-42]\n",
    "        X_t=X[-42:]\n",
    "        y_t=y[-42:]\n",
    "        #Split train,val\n",
    "        X_train=[]\n",
    "        X_val=[]\n",
    "        y_train=[]\n",
    "        y_val=[]\n",
    "        for a in range(3):\n",
    "            X_train.append(X_tr[:126+42*(a)].reshape(-1,1))\n",
    "            y_train.append(y_tr[:126+42*(a)])\n",
    "            X_val.append(X_tr[126+42*(a):126+42*(a+1)].reshape(-1,1))\n",
    "            y_val.append(y_tr[126+42*(a):126+42*(a+1)])\n",
    "        y_t=y_t\n",
    "        X_t=X_t.reshape(-1,1)\n",
    "        yield X_train,X_val,y_train,y_val,y_t,X_t,X_tr,y_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_for_cross_sectional_learning():\n",
    "    X_train=[]\n",
    "    X_val=[]\n",
    "    y_train=[]\n",
    "    y_val=[]\n",
    "    y_t=[]\n",
    "    X_t=[]\n",
    "    X_tr=[]\n",
    "    y_tr=[]\n",
    "    for i in range(len(ticker)):\n",
    "        X_train_tick=[]\n",
    "        X_val_tick=[]\n",
    "        y_train_tick=[]\n",
    "        y_val_tick=[]\n",
    "        y_t_tick=[]\n",
    "        X_t_tick=[]\n",
    "        X_tr_tick=[]\n",
    "        y_tr_tick=[]\n",
    "        a=split_X_y_ticker(i)\n",
    "        for X_train_slide,X_val_slide,y_train_slide,y_val_slide,y_t_slide,X_t_slide,X_tr_slide,y_tr_slide in a:\n",
    "            X_train_tick.append(X_train_slide)\n",
    "            X_val_tick.append(X_val_slide)\n",
    "            y_train_tick.append(y_train_slide)\n",
    "            y_val_tick.append(y_val_slide)\n",
    "            y_t_tick.append(y_t_slide)\n",
    "            X_t_tick.append(X_t_slide)\n",
    "            X_tr_tick.append(X_tr_slide)\n",
    "            y_tr_tick.append(y_tr_slide )        \n",
    "        X_train.append(X_train_tick)\n",
    "        X_val.append(X_val_tick)\n",
    "        y_train.append(y_train_tick)\n",
    "        y_val.append(y_val_tick)\n",
    "        y_t.append(y_t_tick)\n",
    "        X_t.append(X_t_tick)\n",
    "        X_tr.append(X_tr_tick)\n",
    "        y_tr.append(y_tr_tick)\n",
    "    X_train=py.stack([X_train[i] for i in range(len(ticker))],axis=-1)\n",
    "    y_train=py.stack([y_train[i] for i in range(len(ticker))],axis=-1)\n",
    "    X_val=py.stack([X_val[i] for i in range(len(ticker))],axis=-1)\n",
    "    y_val=py.stack([y_val[i] for i in range(len(ticker))],axis=-1)\n",
    "    X_t=py.stack([X_t[i] for i in range(len(ticker))],axis=-1)\n",
    "    y_t=py.stack([y_t[i] for i in range(len(ticker))],axis=-1)\n",
    "    X_tr=py.stack([X_tr[i] for i in range(len(ticker))],axis=-1)\n",
    "    y_tr=py.stack([y_tr[i] for i in range(len(ticker))],axis=-1)\n",
    "    return X_train,X_val,y_train,y_val,y_t,X_t,X_tr,y_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def walk_forward_val(model):#model already defined\n",
    "    Mean_Score_SV=[]\n",
    "    prec=[]\n",
    "    recall=[]\n",
    "    f1=[]\n",
    "    X_train_r,X_val_r,y_train_r,y_val_r,y_t_r,X_t_r,X_tr_r,y_tr_r=compile_for_cross_sectional_learning()\n",
    "    for j in range(len(X_train_r)):\n",
    "        for i in range(len(X_train_r[0])):\n",
    "            X_train=py.concatenate(X_train_r[j][i])\n",
    "            y_train=py.concatenate(y_train_r[j][i])\n",
    "            X_val=X_val_r[j][i].reshape(-1,1)\n",
    "            y_val=y_val_r[j][i].reshape(-1,)\n",
    "            model.fit(X_train,y_train)\n",
    "            y_pred_val=model.predict(X_val)\n",
    "            Mean_Score_SV.append(accuracy_score(y_val,y_pred_val))\n",
    "            prec.append(precision_score(y_val,y_pred_val)) \n",
    "            recall.append(recall_score(y_val,y_pred_val)) #Specificity\n",
    "            f1.append(f1_score(y_val,y_pred_val))\n",
    "    print(py.mean(Mean_Score_SV))\n",
    "    print('')\n",
    "    print(py.mean(prec))\n",
    "    print('')\n",
    "    print(py.mean(recall))\n",
    "    print('')\n",
    "    print(py.mean(f1))\n",
    "    return py.mean(Mean_Score_SV),py.mean(prec),py.mean(recall),py.mean(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_generator(params):\n",
    "    rng = py.random.RandomState(0)\n",
    "    param_list = list(ParameterSampler(params, n_iter=6,random_state=rng))\n",
    "    return param_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_tuning(model,params):\n",
    "    best_score=0\n",
    "    for param in params :\n",
    "        mod=model(**param)\n",
    "        acc,prec,recall, f1= walk_forward_val(mod)\n",
    "        if acc > best_score:\n",
    "            best_score=acc\n",
    "            sensi=recall \n",
    "            preci=prec\n",
    "            best_param=param\n",
    "        print(1)\n",
    "    return best_score,sensi,preci,best_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_predict(model,param):\n",
    "    acc=[]\n",
    "    y_pred_all=[]\n",
    "    mod=model(**param)\n",
    "    X_train_r,X_val_r,y_train_r,y_val_r,y_t_r,X_t_r,X_tr_r,y_tr_r=compile_for_cross_sectional_learning()\n",
    "    for i in range(len(X_train_r)):\n",
    "        X_t=X_t_r[i].reshape(-1,1)\n",
    "        y_t=y_t_r[i].reshape(-1,)\n",
    "        X_tr=X_tr_r[i].reshape(-1,1)\n",
    "        y_tr=y_tr_r[i].reshape(-1,)\n",
    "        mod.fit(X_tr,y_tr)\n",
    "        y_pred=mod.predict_proba(X_t)\n",
    "        y_score=mod.predict(X_t)\n",
    "        y_pred_all.append(y_pred[:,-1])\n",
    "        acc.append(accuracy_score(y_t,y_score))\n",
    "    return y_pred_all,acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score,log_loss,accuracy_score,precision_score,recall_score,f1_score\n",
    "from sklearn.model_selection import ParameterSampler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramsRF={'bootstrap':[True], 'n_estimators':[20,100,180], \n",
    "           'max_depth':[3,5,10],'min_samples_split':[2,4,6],\n",
    "           'min_samples_leaf':[1,4,8]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_scoreRF,sensiRF,speciiRF,best_paramRF=param_tuning(RandomForestClassifier,rand_generator(paramsRF))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_scoreRF,sensiRF,speciiRF,best_paramRF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_allRF,accRF=model_predict(RandomForestClassifier,best_paramRF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(y_pred_allRF).to_csv('proba_RF1.csv')\n",
    "#pd.DataFrame(log).to_csv('logRF.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramsXG={'learning_rate':[0.01,0.05,0.1,0.3],'max_depth': [3,5,10],'gamma': [0,0.1,0.2,1],'reg_alpha' : [1,20,40],\n",
    "           'reg_lambda' : [0,1],'colsample_bytree' : [0.5,0.9],'min_child_weight' : [1,2,6],\n",
    "           'n_estimators': [20,100,180],'seed': [27],'eval_metric':['mlogloss']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_scoreXG,sensiXG,speciiXG,paramXG=param_tuning(XGBClassifier,rand_generator(paramsXG))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_scoreXG,speciiXG,sensiXG,paramXG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_allXG,accXG=model_predict(XGBClassifier,paramXG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(y_pred_allXG).to_csv('proba_XG1.csv')\n",
    "#pd.DataFrame(logXG).to_csv('loXG.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "#from sklearn.calibration import calibration_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramsLR={'C':[0.01,1,100],'solver':['newton-cg', 'lbfgs', 'liblinear'],'penalty':['l2']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_scoreLR,speciLR,sensiLR,paramLR=param_tuning(LogisticRegression,rand_generator(paramsLR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_scoreLR,speciLR,sensiLR,paramLR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_allSV,accSV=model_predict(LogisticRegression, paramLR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(y_pred_allSV).to_csv('proba_LR1.csv')\n",
    "#pd.DataFrame(logSV).to_csv('logSV.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(list(zip(accXG,accSV))).to_csv('proba1.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
